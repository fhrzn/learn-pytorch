{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "\n",
    "# visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# data processing\n",
    "import numpy as np\n",
    "import string\n",
    "import unicodedata\n",
    "import random\n",
    "\n",
    "# utils\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constant\n",
    "BATCH_SIZE = 64\n",
    "EPOCH = 10000\n",
    "LEARNING_RATE = 0.001\n",
    "TEST_SIZE = 0.2\n",
    "DATA_PATH = './data/names/'\n",
    "ALL_LETTERS = string.ascii_letters + '.,;'\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loader\n",
    "def load_data(path):\n",
    "\n",
    "    def to_ascii(text):\n",
    "        return ''.join(c for c in unicodedata.normalize('NFD', text) \\\n",
    "            if unicodedata.category(c) != 'Mn' and c in ALL_LETTERS)\n",
    "\n",
    "    files = os.listdir(path)\n",
    "    names, labels = [], []   # declare empty list for X and y\n",
    "    all_labels = []\n",
    "\n",
    "    for f in files:\n",
    "        label = f.split('.')[0]\n",
    "        all_labels.append(label)\n",
    "\n",
    "        with open(path + f, encoding='utf-8') as r:\n",
    "            lines = r.read().strip().split('\\n')    # read each record\n",
    "            \n",
    "            names.extend([to_ascii(l) for l in lines])\n",
    "            labels.extend([label for _ in range(len(lines))])    \n",
    "\n",
    "    # convert to numpy\n",
    "    names = np.array(names)\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    # shuffle data\n",
    "    index = np.arange(len(labels))\n",
    "    np.random.shuffle(index)\n",
    "    names = names[index]\n",
    "    labels = labels[index]\n",
    "\n",
    "    return names, labels, all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['Kouma', 'Makhmudov', 'Kabyshev', 'Zimitsky', 'Spyridis'],\n",
       "       dtype='<U19'),\n",
       " array(['English', 'Russian', 'Russian', 'Russian', 'Greek'], dtype='<U10'))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y, all_labels = load_data(DATA_PATH)\n",
    "X[:5], y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data encoder\n",
    "def onehot_encode(data, device=None):\n",
    "\n",
    "    def _char_encode(char):\n",
    "        return ALL_LETTERS.index(char)\n",
    "    \n",
    "    def _line_encode(line):\n",
    "        return np.array([_char_encode(c) for c in line])\n",
    "\n",
    "    def single_onehot_encode(s, device=None):\n",
    "        onehot = torch.zeros((len(s), len(ALL_LETTERS)), dtype=torch.float32, device=device) \\\n",
    "            if device else torch.zeros((len(s), len(ALL_LETTERS)), dtype=torch.float32)\n",
    "        \n",
    "        for i,char in enumerate(s):\n",
    "            onehot[i][_char_encode(char)] = 1\n",
    "                        \n",
    "        return onehot\n",
    "\n",
    "    \n",
    "    # all_onehot = []\n",
    "    # for d in data:\n",
    "    #     onehot = single_onehot_encode(d)        \n",
    "    #     all_onehot.append(onehot)\n",
    "\n",
    "    return single_onehot_encode(data, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0.],\n",
       "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0.],\n",
       "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0.]])"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc = onehot_encode('Affandy')\n",
    "enc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(x, y, test_size=.2):\n",
    "    npx = np.array(x)\n",
    "    npy = np.array(y)\n",
    "    # get test index\n",
    "    test_id = np.random.choice(range(len(y)), int(test_size * len(y)))\n",
    "    # test data\n",
    "    x_test = npx[np.isin(np.arange(len(y)), test_id)]\n",
    "    y_test = npy[np.isin(np.arange(len(y)), test_id)]\n",
    "    # train data\n",
    "    x_train = npx[~np.isin(np.arange(len(y)), test_id)]\n",
    "    y_train = npy[~np.isin(np.arange(len(y)), test_id)]\n",
    "    \n",
    "    return zip(x_train, y_train), zip(x_test, y_test)\n",
    "\n",
    "def train_test_split_index(x, y, test_size=.2):\n",
    "    return np.random.choice(range(len(y)), int(test_size * len(y)))\n",
    "\n",
    "def get_batches(arr, batch_size):\n",
    "\n",
    "    feature, label = arr\n",
    "    # feature = onehot_encode(feature)\n",
    "    # label = torch.tensor([all_labels.index()])\n",
    "\n",
    "    for i in range(0, len(feature), batch_size):\n",
    "        x = feature[i:i+batch_size]\n",
    "        y = label[i:i+batch_size]\n",
    "\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(X, y, test_size=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = get_batches(train, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list, numpy.ndarray)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tx, ty = next(batch)\n",
    "type(tx[:5]), type(ty[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_to_label(output):\n",
    "    label_id = torch.argmax(output).item()\n",
    "    return all_labels[label_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        # from input layer to hidden layer\n",
    "        self.layer_hidden = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        # from input layer to output layer\n",
    "        self.layer_output = nn.Linear(input_size + hidden_size, output_size)\n",
    "        # softmax activation since we want multiclass classification\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input_tensor, hidden_tensor):\n",
    "        # combine input and hidden\n",
    "        combined = torch.cat((input_tensor, hidden_tensor), dim=1)\n",
    "\n",
    "        # forward pass to hidden layer\n",
    "        hidden = self.layer_hidden(combined)\n",
    "        # forward pass to output layer\n",
    "        output = self.layer_output(combined)\n",
    "        # apply softmax to output\n",
    "        output = self.softmax(output)\n",
    "\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self, device=None):\n",
    "        # hidden state weight initialization        \n",
    "        return torch.zeros((1, self.hidden_size), device=device) if device else torch.zeros(1, self.hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(model, train_data, val_data, device, epochs=1000, learning_rate=0.005, print_every=100):    \n",
    "    # set model to train mode\n",
    "    model.train()\n",
    "    # define optimizer\n",
    "    optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "    # define loss function\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    # move to device (CPU/GPU)\n",
    "    model.to(device)\n",
    "\n",
    "    # define loss and acc\n",
    "    train_loss, val_loss = [], []\n",
    "    train_acc, val_acc = [], []\n",
    "\n",
    "    # validation loss minimum\n",
    "    val_loss_min = np.Inf\n",
    "\n",
    "    for e in tqdm(range(epochs), ncols=75, desc='Training'):\n",
    "\n",
    "        ####################\n",
    "        # Training Section #\n",
    "        ####################\n",
    "\n",
    "        # define local loss and acc\n",
    "        tloss = 0\n",
    "        tacc = []\n",
    "\n",
    "        # assume we're using batch_size=1\n",
    "        for feature, target in train_data:\n",
    "            # init hidden state\n",
    "            hidden = model.init_hidden(device)      \n",
    "\n",
    "            # transform dataset\n",
    "            feature = onehot_encode(feature, device)\n",
    "            target = torch.tensor([all_labels.index(target)], dtype=torch.long, device=device)\n",
    "\n",
    "            # move to device\n",
    "            # feature, target = feature.to(device), target.to(device)            \n",
    "            # reset optimizer\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # forward pass for each character sequences\n",
    "            for row in feature:\n",
    "                # add dimension to feature (in order to concat with hidden tensor)\n",
    "                row = row.view(1,-1)\n",
    "                output, hidden = model(row, hidden)\n",
    "            \n",
    "            # compute loss\n",
    "            loss = criterion(output, target)\n",
    "            # record loss\n",
    "            tloss += loss.item()\n",
    "\n",
    "            # compute accuracy\n",
    "            predicted = output_to_label(output)\n",
    "            tacc.append(True if predicted == target else False)\n",
    "\n",
    "            # backpropagation\n",
    "            loss.backward()\n",
    "            # update weights\n",
    "            optimizer.step()\n",
    "\n",
    "        # record to global loss and accuracy\n",
    "        train_loss.append(tloss/len(train_data))\n",
    "        train_acc.append(np.mean(tacc))\n",
    "\n",
    "\n",
    "        ######################\n",
    "        # Validation Section #\n",
    "        ######################\n",
    "\n",
    "        # define local loss and acc\n",
    "        vloss = 0\n",
    "        vacc = []\n",
    "\n",
    "        # set model to eval mode\n",
    "        model.eval()\n",
    "\n",
    "        # we turned off gradient on validation mode\n",
    "        with torch.no_grad():\n",
    "            # assume batch_size=1\n",
    "            for feature, target in val_data:\n",
    "                # init hidden state\n",
    "                hidden = model.init_hidden(device)      \n",
    "\n",
    "                # transform dataset\n",
    "                feature = onehot_encode(feature, device)\n",
    "                target = torch.tensor([all_labels.index(target)], dtype=torch.long, device=device)\n",
    "\n",
    "                # move to device\n",
    "                feature, target = feature.to(device), target.to(device)            \n",
    "                # reset optimizer\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # forward pass for each character sequences\n",
    "                for row in feature:\n",
    "                    output, hidden = model(row, hidden)\n",
    "\n",
    "                # compute loss\n",
    "                loss = criterion(output, target)\n",
    "                # record loss\n",
    "                vloss += loss.item()\n",
    "                \n",
    "                # compute accuracy\n",
    "                predicted = output_to_label(output)\n",
    "                vacc.append(True if predicted == target else False)\n",
    "\n",
    "            # record to global loss and accuracy\n",
    "            val_loss.append(vloss/len(val_data))\n",
    "            val_acc.append(np.mean(vacc))\n",
    "        \n",
    "        # reset model to train mode\n",
    "        model.train()\n",
    "\n",
    "        # print epoch\n",
    "        if (e+1) % print_every == 0:\n",
    "            print(f\"Loss: {tloss/len(train_data)} - Val Loss: {vloss/len(val_data)} Acc: {np.mean(vacc)}\")\n",
    "                \n",
    "        # save model if validation loss decrease\n",
    "        if vloss/len(val_data) <= val_loss_min:            \n",
    "            torch.save(model.state_dict(), 'rnn.pt')\n",
    "        else:\n",
    "            print(f'Validation loss not improving ({val_loss_min:.6f} --> {vloss/len(val_data)})')\n",
    "\n",
    "    return train_loss, val_loss, train_acc, val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "X, y, all_labels = load_data(DATA_PATH)\n",
    "# split into train and test\n",
    "train, test = train_test_split(X, y)\n",
    "# split into train and val with 8:2 ratio\n",
    "train, val = train_test_split(X, y, .2)\n",
    "\n",
    "# define network\n",
    "hidden_size = 128\n",
    "rnn = RNN(len(ALL_LETTERS), hidden_size, len(all_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|                                   | 0/1000 [01:21<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'zip' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-180-c3d4eaa5ec3c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# train\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtraining\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrnn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDEVICE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-177-492a59774b7e>\u001b[0m in \u001b[0;36mtraining\u001b[1;34m(model, train_data, val_data, device, epochs, learning_rate, print_every)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[1;31m# record to global loss and accuracy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m         \u001b[0mtrain_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtloss\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m         \u001b[0mtrain_acc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtacc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'zip' has no len()"
     ]
    }
   ],
   "source": [
    "# train\n",
    "training(rnn, train, val, DEVICE)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "49a3e1d03718a75483d59e7098aed45d369cd6c424a1fdf18bcb17a881710a11"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
